"""
æ™ºèƒ½å›å¤Action - å®˜æ–¹æ–‡æ¡£é«˜çº§ç‰¹æ€§ç¤ºä¾‹

å±•ç¤ºå†…å®¹ï¼š
- éšæœºæ¿€æ´»æœºåˆ¶
- LLM_JUDGEæ™ºèƒ½åˆ¤æ–­
- é…ç½®é©±åŠ¨çš„å¤æ‚è¡Œä¸º
- æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ
- ç¼“å­˜æœºåˆ¶çš„ä½¿ç”¨
"""

from src.plugin_system import BaseAction, ActionActivationType, ChatMode
from typing import Tuple
import random
import time


class SmartResponseAction(BaseAction):
    """
    æ™ºèƒ½å›å¤Action
    
    å±•ç¤ºé«˜çº§Actionç‰¹æ€§ï¼š
    - å¤šç§æ¿€æ´»ç±»å‹ç»„åˆä½¿ç”¨
    - å¤æ‚çš„é…ç½®é©±åŠ¨è¡Œä¸º
    - æ€§èƒ½ä¼˜åŒ–å’Œç¼“å­˜
    - æ™ºèƒ½å†…å®¹ç”Ÿæˆ
    """
    
    # ==================== æ¿€æ´»æ§åˆ¶ ====================
    # ä¸“æ³¨æ¨¡å¼ï¼šLLMæ™ºèƒ½åˆ¤æ–­
    focus_activation_type = ActionActivationType.LLM_JUDGE
    
    # æ™®é€šæ¨¡å¼ï¼šéšæœºæ¿€æ´»ï¼Œå¢åŠ è¡Œä¸ºéšæœºæ€§
    normal_activation_type = ActionActivationType.RANDOM
    
    # æ‰€æœ‰æ¨¡å¼ä¸‹éƒ½å¯æ¿€æ´»
    mode_enable = ChatMode.ALL
    
    # å…è®¸ä¸å…¶ä»–Actionå¹¶è¡Œæ‰§è¡Œ
    parallel_action = True
    
    # ==================== éšæœºæ¿€æ´»é…ç½® ====================
    # æ¦‚ç‡å°†ä»é…ç½®ä¸­åŠ¨æ€è·å–
    random_activation_probability = 0.1  # é»˜è®¤10%æ¦‚ç‡
    
    # ==================== LLMåˆ¤æ–­é…ç½® ====================
    llm_judge_prompt = """
    åˆ¤å®šæ˜¯å¦éœ€è¦ä½¿ç”¨æ™ºèƒ½å›å¤åŠ¨ä½œçš„æ¡ä»¶ï¼š
    1. ç”¨æˆ·æå‡ºäº†é—®é¢˜æˆ–éœ€è¦å»ºè®®
    2. å¯¹è¯å†…å®¹æ¯”è¾ƒå¤æ‚ï¼Œéœ€è¦æ·±å…¥å›åº”
    3. ç”¨æˆ·è¡¨è¾¾äº†æƒ…ç»ªï¼Œéœ€è¦æƒ…æ„Ÿæ”¯æŒ
    4. å½“å‰å›å¤å¯èƒ½ä¸å¤Ÿå……åˆ†ï¼Œéœ€è¦è¡¥å……
    5. ç”¨æˆ·åœ¨è®¨è®ºä¸“ä¸šè¯é¢˜ï¼Œéœ€è¦ç›¸å…³çŸ¥è¯†
    
    æ’é™¤æ¡ä»¶ï¼š
    1. ç”¨æˆ·åªæ˜¯ç®€å•æ‰“æ‹›å‘¼
    2. å¯¹è¯åˆšå¼€å§‹ï¼Œä¿¡æ¯è¿˜ä¸å¤Ÿ
    3. å·²ç»æœ‰å…¶ä»–Actionåœ¨å¤„ç†
    
    è¯·å›ç­”"æ˜¯"æˆ–"å¦"ã€‚
    """
    
    # ==================== åŸºæœ¬ä¿¡æ¯ ====================
    action_name = "smart_response_action"
    action_description = "æ™ºèƒ½å›å¤Actionï¼Œæ ¹æ®å¯¹è¯å†…å®¹ç”Ÿæˆç›¸å…³çš„è¡¥å……å›å¤"
    
    # ==================== åŠŸèƒ½å®šä¹‰ ====================
    action_parameters = {
        "response_type": "å›å¤ç±»å‹ï¼ˆinformative/supportive/creative/analyticalï¼‰",
        "context_depth": "ä¸Šä¸‹æ–‡æ·±åº¦ï¼ˆshallow/medium/deepï¼‰",
        "tone": "å›å¤è¯­è°ƒï¼ˆformal/casual/friendly/professionalï¼‰",
        "max_length": "æœ€å¤§å›å¤é•¿åº¦",
        "include_examples": "æ˜¯å¦åŒ…å«ç¤ºä¾‹ï¼ˆtrue/falseï¼‰"
    }
    
    action_require = [
        "å½“ç”¨æˆ·éœ€è¦é¢å¤–ä¿¡æ¯æˆ–æ”¯æŒæ—¶ä½¿ç”¨",
        "é€‚åˆè¡¥å……å¯¹è¯ä¸­çš„ç©ºç™½ä¿¡æ¯",
        "å¯ä»¥æä¾›ç›¸å…³çš„å»ºè®®æˆ–è§è§£",
        "é¿å…ä¸ä¸»è¦å›å¤é‡å¤",
        "ä¿æŒå›å¤çš„ç›¸å…³æ€§å’Œæœ‰ç”¨æ€§",
        "ä¸è¦åœ¨ç®€å•å¯¹è¯ä¸­è¿‡åº¦ä½¿ç”¨"
    ]
    
    associated_types = ["text", "image"]
    
    # ==================== ç¼“å­˜å’Œæ€§èƒ½ ====================
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._response_cache = {}
        self._last_cache_clear = time.time()
    
    # ==================== æ‰§è¡Œé€»è¾‘ ====================
    async def execute(self) -> Tuple[bool, str]:
        """
        æ‰§è¡Œæ™ºèƒ½å›å¤
        
        å±•ç¤ºé«˜çº§åŠŸèƒ½ï¼š
        - ç¼“å­˜æœºåˆ¶ä½¿ç”¨
        - æ€§èƒ½ç›‘æ§
        - å¤æ‚é…ç½®å¤„ç†
        - æ™ºèƒ½å†…å®¹ç”Ÿæˆ
        """
        try:
            start_time = time.time()
            
            # æ£€æŸ¥åŠŸèƒ½æ˜¯å¦å¯ç”¨
            if not self.get_config("features.enable_smart_responses", True):
                return False, "æ™ºèƒ½å›å¤åŠŸèƒ½å·²ç¦ç”¨"
            
            # è·å–é…ç½®
            response_probability = self.get_config("actions.response_probability", 0.1)
            max_response_length = self.get_config("actions.max_response_length", 200)
            cache_enabled = self.get_config("advanced.cache_enabled", True)
            performance_monitor = self.get_config("advanced.performance_monitor", False)
            debug_mode = self.get_config("plugin.debug_mode", False)
            
            # æ›´æ–°éšæœºæ¿€æ´»æ¦‚ç‡ï¼ˆé…ç½®é©±åŠ¨ï¼‰
            self.random_activation_probability = response_probability
            
            # è·å–Actionå‚æ•°
            response_type = self.action_data.get("response_type", "informative")
            context_depth = self.action_data.get("context_depth", "medium")
            tone = self.action_data.get("tone", "friendly")
            max_length = self.action_data.get("max_length", max_response_length)
            include_examples = self.action_data.get("include_examples", False)
            
            # è°ƒè¯•ä¿¡æ¯
            if debug_mode:
                await self.send_text(
                    f"ğŸ¤– æ™ºèƒ½å›å¤ï¼šç±»å‹={response_type}, æ·±åº¦={context_depth}, è¯­è°ƒ={tone}"
                )
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"{response_type}_{context_depth}_{tone}"
            if cache_enabled and cache_key in self._response_cache:
                cached_response = self._response_cache[cache_key]
                await self.send_text(cached_response)
                
                if debug_mode:
                    await self.send_text("ğŸ“‹ ä½¿ç”¨äº†ç¼“å­˜å›å¤")
                
                return True, "å‘é€äº†ç¼“å­˜çš„æ™ºèƒ½å›å¤"
            
            # æ¸…ç†è¿‡æœŸç¼“å­˜
            if cache_enabled:
                await self._cleanup_expired_cache()
            
            # ç”Ÿæˆæ™ºèƒ½å›å¤
            smart_response = await self._generate_smart_response(
                response_type, context_depth, tone, max_length, include_examples
            )
            
            # é™åˆ¶é•¿åº¦
            if len(smart_response) > max_length:
                smart_response = smart_response[:max_length] + "..."
            
            # å‘é€å›å¤
            await self.send_text(smart_response)
            
            # æ›´æ–°ç¼“å­˜
            if cache_enabled:
                self._response_cache[cache_key] = smart_response
            
            # è®°å½•åŠ¨ä½œä¿¡æ¯
            await self.store_action_info(
                action_build_into_prompt=True,
                action_prompt_display=f"æä¾›äº†æ™ºèƒ½å›å¤ï¼š{smart_response[:30]}...",
                action_done=True,
                response_type=response_type,
                response_length=len(smart_response),
                from_cache=cache_key in self._response_cache
            )
            
            # æ€§èƒ½ç›‘æ§
            if performance_monitor:
                execution_time = time.time() - start_time
                if execution_time > 2.0:  # å¦‚æœæ‰§è¡Œæ—¶é—´è¶…è¿‡2ç§’
                    await self.send_text(f"âš ï¸ æ€§èƒ½è­¦å‘Šï¼šæ‰§è¡Œæ—¶é—´ {execution_time:.2f}s")
            
            return True, f"æˆåŠŸç”Ÿæˆæ™ºèƒ½å›å¤ï¼Œç±»å‹ï¼š{response_type}"
            
        except Exception as e:
            error_msg = f"æ™ºèƒ½å›å¤Actionæ‰§è¡Œå¤±è´¥: {str(e)}"
            
            if self.get_config("plugin.debug_mode", False):
                await self.send_text(f"âŒ {error_msg}")
            
            return False, error_msg
    
    # ==================== ç§æœ‰æ–¹æ³• ====================
    async def _generate_smart_response(self, response_type: str, context_depth: str, 
                                     tone: str, max_length: int, include_examples: bool) -> str:
        """
        ç”Ÿæˆæ™ºèƒ½å›å¤å†…å®¹
        
        æ ¹æ®å‚æ•°ç”Ÿæˆä¸åŒç±»å‹çš„å›å¤
        """
        # åŸºç¡€å›å¤æ¨¡æ¿
        response_templates = {
            "informative": [
                "å…³äºè¿™ä¸ªè¯é¢˜ï¼Œæˆ‘äº†è§£åˆ°ä¸€äº›æœ‰è¶£çš„ä¿¡æ¯...",
                "è¡¥å……ä¸€ä¸‹ç›¸å…³çš„èƒŒæ™¯çŸ¥è¯†ï¼š",
                "è¿™è®©æˆ‘æƒ³åˆ°äº†ä¸€äº›ç›¸å…³çš„å†…å®¹ï¼š"
            ],
            "supportive": [
                "æˆ‘ç†è§£ä½ çš„æƒ³æ³•ï¼Œè¿™ç¡®å®æ˜¯ä¸€ä¸ªå€¼å¾—è€ƒè™‘çš„é—®é¢˜ã€‚",
                "ä½ çš„è§‚ç‚¹å¾ˆæœ‰æ„æ€ï¼Œæˆ‘æƒ³åˆ†äº«ä¸€äº›ç›¸å…³çš„æƒ³æ³•ï¼š",
                "ä»å¦ä¸€ä¸ªè§’åº¦æ¥çœ‹ï¼Œä¹Ÿè®¸å¯ä»¥è¿™æ ·è€ƒè™‘ï¼š"
            ],
            "creative": [
                "è¿™æ¿€å‘äº†æˆ‘çš„ä¸€äº›åˆ›æ„æƒ³æ³•ï¼š",
                "ä»åˆ›æ„çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·æ€è€ƒï¼š",
                "è®©æˆ‘åˆ†äº«ä¸€ä¸ªæœ‰è¶£çš„æƒ³æ³•ï¼š"
            ],
            "analytical": [
                "ä»åˆ†æçš„è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªé—®é¢˜æœ‰å‡ ä¸ªç»´åº¦ï¼š",
                "è®©æˆ‘ä»¬æ·±å…¥åˆ†æä¸€ä¸‹è¿™ä¸ªæƒ…å†µï¼š",
                "æ•°æ®è¡¨æ˜è¿™ç§ç°è±¡èƒŒåå¯èƒ½æœ‰ä»¥ä¸‹åŸå› ï¼š"
            ]
        }
        
        # é€‰æ‹©åŸºç¡€æ¨¡æ¿
        templates = response_templates.get(response_type, response_templates["informative"])
        base_response = random.choice(templates)
        
        # æ ¹æ®è¯­è°ƒè°ƒæ•´
        if tone == "formal":
            base_response = base_response.replace("æˆ‘", "æœ¬ç³»ç»Ÿ").replace("ä½ ", "æ‚¨")
        elif tone == "casual":
            base_response += " ğŸ˜Š"
        
        # æ·»åŠ å†…å®¹æ·±åº¦
        if context_depth == "deep":
            base_response += "\n\næ·±å…¥æ¥çœ‹ï¼Œè¿™ä¸ªè¯é¢˜æ¶‰åŠå¤šä¸ªå±‚é¢çš„è€ƒè™‘..."
        elif context_depth == "medium":
            base_response += "\n\nè¿™å…¶ä¸­æœ‰ä¸€äº›é‡è¦çš„è¦ç‚¹å€¼å¾—æ³¨æ„ã€‚"
        
        # æ·»åŠ ç¤ºä¾‹
        if include_examples:
            base_response += "\n\nä¸¾ä¸ªä¾‹å­æ¥è¯´..."
        
        return base_response
    
    async def _cleanup_expired_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        current_time = time.time()
        cache_ttl = self.get_config("advanced.cache_ttl", 3600)
        
        # æ¯å°æ—¶æ¸…ç†ä¸€æ¬¡
        if current_time - self._last_cache_clear > cache_ttl:
            self._response_cache.clear()
            self._last_cache_clear = current_time
            
            if self.get_config("plugin.debug_mode", False):
                await self.send_text("ğŸ—‘ï¸ å·²æ¸…ç†è¿‡æœŸç¼“å­˜")
